{"abs":"We introduce the notion of expected hitting time to a goal as a measure of the con- vergence rate of a Monte Carlo optimization method. The techniques developed apply to Simulated Annealing, Genetic Algorithms and other stochastic search schemes. The expected hitting time can itself be calculated from the more fundamental complementary hitting time distribution (CHTD) which completely characterizes a Monte Carlo method. The CHTD is asymptotically a geometric series, (1/s)/(1-lambda), characterized by two parameters, s, lambda, related to the search process in a simple way. The main utility of the CHTD is in comparing Monte Carlo algorithms. In particular we show that independent, identical Monte Carlo algorithms run in parallel, IIP parallelism, exhibit superlinear speedup. We give conditions under which this occurs and note that equally likely search is linearly spedup. Further we observe that a serial Monte Carlo search can have in nite expected hitting time, but the same algorithm when parallelized can have nite expected hitting time. One consequence of the observed superlinear speedup is an improved uni-processor algorithm by the technique of in-code parallelism.","title":"Parallel Speed-Up of Monte Carlo Methods for Global Optimization","id":40220797,"url":"https://www.researchgate.net/publication/40220797_Parallel_Speed-Up_of_Monte_Carlo_Methods_for_Global_Optimization","names":["R. Shonkwiler","Erik S. Van Vleck","Journal of Complexity"],"references":{"6026283":"Optimization by Simulated Annealing,‚Äù Science 220, 671-680","205716410":"Matrix Iterative Analysis","201976419":"Cooling Schedules for Optimal Annealing","227227015":"Nonstationary Markov Chains and Convergence of the Annealing Algorithm","220182914":"Geman, D.: Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images. IEEE Trans. Pattern Anal. Mach. Intell. PAMI-6(6), 721-741","44478088":"Non-negative matrices and markov chains / E. Seneta","30870312":"Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesley, Boston, MA"},"citedIn":{"251753824":"Optimization by Stochastic Methods","222487907":"A Feedback Algorithm for Determining Search Parameters for Monte Carlo Optimization","2757000":"Approximate Speedup by Independent Identical Processing","241131452":"Geometric convergence of genetic algorithms under tempered random restart","253007440":"Simple Parallel Genetic Algorithm Using Cloud Computing","277291874":"Running head: Iterative Improvement plus Random Restart Mail proofs to:","266338201":"Fractal Joint Signatures","235178286":"Increased UAV Task Assignment Performance Through Parallelized Genetic Algorithms (Preprint)","221149605":"Parallel Search for Combinatorial Optimization: Genetic Algorithms, Simulated Annealing, Tabu Search and GRASP.","2759169":"Parallel Search for Combinatorial Optimization: Genetic Algorithms, Simulated Annealing, Tabu Search and GRASP","2415592":"Annealing a Genetic Algorithm over Constraints","226295121":"Analysis of Random Restart and Iterated Improvement for Global Optimization with Application to the Traveling Salesman Problem","222455084":"Efficient hybrid search for visual reconstruction problems","3515717":"Parallel simulated annealing for the n-queen problem","2453264":"Parallel Metaheuristics For Combinatorial Optimization","238878122":"Faster Simulated Annealing","227051921":"Parallel cooperative propositional theorem proving","2327034":"Heavy-Tailed Phenomena in Satisfiability and Constraint Satisfaction Problems","255574016":"Estimating the Convergence Rate of a Restarted Search Process","225216432":"Parallel algorithms for global optimization problems","3555915":"Genetic algorithm/neural network synergy for nonlinearly constrained optimization problems","40220792":"Random Restarts in Global Optimization","266409086":"Simulated Annealing: Folklore, Facts, and Directions","202051284":"Online Dynamic Algorithm Portfolios"},"index":960}