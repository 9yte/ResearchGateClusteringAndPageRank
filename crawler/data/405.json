{"abs":"Recent implementations of local approximate Gaussian process models have pushed computational boundaries for non-linear, non-parametric prediction problems, particularly when deployed as emulators for computer experiments. Their flavor of spatially independent computation accommodates massive parallelization, meaning that they can handle designs two or more orders of magnitude larger than previously. However, accomplishing that feat can still require massive supercomputing resources. Here we aim to ease that burden. We study how predictive variance is reduced as local designs are built up for prediction. We then observe how the exhaustive and discrete nature of an important search subroutine involved in building such local designs may be overly conservative. Rather, we suggest that searching the space radially, i.e., continuously along rays emanating from the predictive location of interest, is a far thriftier alternative. Our empirical work demonstrates that ray-based search yields predictors with accuracy comparable to exhaustive search, but in a fraction of the time - bringing a supercomputer implementation back onto the desktop.","title":"Speeding up neighborhood search in local Gaussian process prediction","id":265252310,"url":"https://www.researchgate.net/publication/265252310_Speeding_up_neighborhood_search_in_local_Gaussian_process_prediction","names":["Robert B. Gramacy","Benjamin Haaland","Technometrics"],"references":{"4993358":"Fixed rank Kriging for very large data sets","266856357":"Calibrating a large computer experiment simulating radiative shock hydrodynamics","51932830":"Variable selection and sensitivity analysis via dynamic trees with an application to computer code performance tuning","4914189":"Approximating likelihoods for large spatial data sets","221933053":"Interpolation of Spatial Data","239427784":"Modeling Data from Computer Experiments: An Empirical Comparison of Kriging with MARS and Projection Pursuit Regression","247598156":"Sparse Gaussian Process Using Pseudo-inputs","227694402":"A full scale approximation of covariance functions for large spatial data sets","263545382":"Hierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets","257016708":"The effect of the nugget on Gaussian process emulators of computer models","236871817":"Parallelizing Gaussian Process Calculations in R","221669294":"A Short Note on Gaussian Process Modeling for Large Datasets using\nGraphics Processing Units","239886805":"Bayesian Design and Analysis of Computer Experiments: Use of Derivatives in Surface Prediction","35411845":"Adaptive importance sampling for integration /","37121135":"Algorithms For Minimization Without Derivatives","246097585":"Estimation and Identification for Continuous Spatial Processes","221669355":"Sequential Design for Computer Experiments with a Flexible Bayesian\nAdditive Model","242235985":"Accurate emulators for large-scale computer experiments","258082179":"Massively Parallel Approximate Gaussian Process Regression","216301381":"The Design and Analysis Computer Experiments","235765935":"Local Gaussian Process Approximation for Large Computer Experiments","236474812":"Deterministic uncertainty analysis","4743918":"Objective Bayesian Analysis of Spatially Correlated Data","38363493":"[Design and Analysis of Computer Experiments]: Rejoinder","51914869":"Efficient emulators of computer experiments using compactly supported correlation functions, with application to cosmology","1733357":"Adaptive Design and Analysis of Supercomputer Experiments","45875211":"Particle Learning of Gaussian Process Models for Sequential Design and Optimization","3857869":"Gaussian Process Regression: Active Data Selection and Test Point Rejection","45931398":"Cases for the Nugget in modelling computer experiments","259440008":"Modern Industrial Statistics: Design and Control of Quality and Reliability"},"citedIn":{},"index":405}