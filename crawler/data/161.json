{"abs":"Inverse problems lend themselves naturally to a Bayesian formulation, in which the quantity of interest is a posterior distribution of state and/or parameters given some uncertain observations. For the common case in which the forward operator is smoothing, then the inverse problem is ill-posed. Well-posedness is imposed via regularization in the form of a prior, which is often Gaussian. Under quite general conditions, it can be shown that the posterior is absolutely continuous with respect to the prior and it may be well-defined on function space in terms of its density with respect to the prior. In this case, by constructing a proposal for which the prior is invariant, one can define Metropolis-Hastings schemes for MCMC which are well-defined on function space (Stuart (2010) [1], Cotter et al. [2]), and hence do not degenerate as the dimension of the underlying quantity of interest increases to infinity, e.g. under mesh refinement when approximating PDE in finite dimensions. However, in practice, despite the attractive theoretical properties of the currently available schemes, they may still suffer from long correlation times, particularly if the data is very informative about some of the unknown parameters. In fact, in this case it may be the directions of the posterior which coincide with the (already known) prior which decorrelate the slowest. The information incorporated into the posterior through the data is often contained within some finite-dimensional subspace, in an appropriate basis, perhaps even one defined by eigenfunctions of the prior. We aim to exploit this fact and improve the mixing time of function-space MCMC by careful rescaling of the proposal. To this end, we introduce two new basic methods of increasing complexity, involving (i) characteristic function truncation of high frequencies and (ii) Hessian information to interpolate between low and high frequencies. The second, more sophisticated version, bears some similarities with recent methods which exploit local curvature information, for example RMHMC, Girolami and Calderhead (2011) [3], and stochastic Newton, Martin et al. (2012) [4]. These ideas are illustrated with numerical experiments on the Bayesian inversion of the heat equation and Navier-Stokes equation, given noisy observations.","title":"Proposals which speed up function-space MCMC","id":267124979,"url":"https://www.researchgate.net/publication/267124979_Proposals_which_speed_up_function-space_MCMC","names":["Kody Law","Journal of Computational and Applied Mathematics"],"references":{"239546247":"MCMC methods for diffusion bridges","221661751":"MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster","246546576":"MAP Estimators and Their Consistency in Bayesian Nonparametric Inverse Problems","2358401":"Exponential Convergence of Langevin Diffusions and Their Discrete Approximations","200622065":"Equation of State Calculations by Fast Computing Machines","243104985":"Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms. J Appl Probab","245481576":"Inverse problems: A Bayesian perspective","38373539":"Exponential Convergence of Langevin Distributions and Their Discrete Approximations","227375782":"Riemann manifold Langevin and Hamiltonian Monte Carlo methods","235726913":"The Bayesian Approach To Inverse Problems","267473441":"A Stochastic Newton MCMC Method for Large-Scale Statistical Inverse Problems with Application to Seismic Inversion","229099827":"General Methods for Monitoring Convergence of Iterative Simulations","2269174":"A Note on Metropolis-Hastings Kernels for General State Spaces","31403274":"Monte Carlo Sampling Methods Using Markov Chains and Their Application","51969792":"Spectral Gaps for a Metropolis-Hastings Algorithm in Infinite Dimensions","2583743":"Weak Convergence And Optimal Scaling Of Random Walk Metropolis Algorithms","239384155":"General Methods for Monitoring Convergence of Iterative Simulations","227657870":"Riemann manifold Langevin and Hamiltonian Monte Carlo methods. J. R. Stat. Soc. B 73, 123-214"},"citedIn":{"275055333":"On a generalization of the preconditioned Crank-Nicolson Metropolis algorithm","279460377":"On exact posterior distributions using H-functions","268227549":"Dimension-independent likelihood-informed MCMC","278733347":"Accelerated dimension-independent adaptive Metropolis","280946496":"An adaptive independence sampler MCMC algorithm for infinite-dimensional bayesian inference","261289021":"Information-Geometric Markov Chain Monte Carlo Methods Using Diffusions"},"index":161}