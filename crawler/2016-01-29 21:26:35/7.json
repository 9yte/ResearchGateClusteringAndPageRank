{"abs":"Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly.","title":"MCMC for Variationally Sparse Gaussian Processes","id":278332447,"url":"https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","names":["James Hensman","Alexander G. de G. Matthews","Maurizio Filippone","Zoubin Ghahramani"],"references":{"257618460":"A comparative evaluation of stochastic-based inference methods for Gaussian process models","45921723":"Slice sampling covariance hyperparameters of latent Gaussian models","41781429":"Assessing Approximate Inference for Binary Gaussian Process Classification","220343914":"Soft Margins for AdaBoost","260089482":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","220499817":"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors","247598156":"Sparse Gaussian Process Using Pseudo-inputs","266502061":"Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression","45893043":"Elliptical Slice Sampling.","271218362":"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)","220320048":"Variational Learning of Inducing Variables in Sparse Gaussian Processes","41781800":"Approximations for Binary Gaussian Process Classification","3192933":"Bayesian classification with Gaussian processes","234779817":"Sparse Spectrum Gaussian Process Regression","275588150":"On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes","267759656":"Variational Inference for Gaussian Process Modulated Poisson Processes","227701452":"Log Gaussian Cox Processes","41781406":"A Unifying View of Sparse Approximate Gaussian Process Regression","259390620":"Bayesian Filtering and Smoothing","235703008":"Adaptive Hamiltonian and Riemann Manifold Monte Carlo Samplers","11500673":"Sparse On-Line Gaussian Processes","259844876":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression","268079368":"Scalable Variational Gaussian Process Classification","267454466":"Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes","2795225":"Variational Gaussian Process Classifiers","262348245":"Variational Multinomial Logit Gaussian Process","221618434":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine","6690142":"Bayesian Gaussian Process Classification with the EM-EP Algorithm","239030086":"Differentiation of the Cholesky Algorithm","2834582":"Scaling Limits for the Transient Phase of Local Metropolis-Hastings Algorithms","23252767":"The Variational Gaussian Approximation Revisited","220320094":"Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology."},"citedIn":{"280773011":"Adaptive Multiple Importance Sampling for Gaussian Processes"},"index":7}